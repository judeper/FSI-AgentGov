# Control 2.20: Adversarial Testing and Red Team Framework

**Control ID:** 2.20
**Pillar:** Management
**Regulatory Reference:** OCC 2011-12, Fed SR 11-7, FINRA 25-07, NIST AI RMF, MITRE ATLAS
**Last UI Verified:** January 2026
**Governance Levels:** Baseline / Recommended / Regulated

---

## Objective

Establish a proactive adversarial testing program to identify vulnerabilities in AI agents before deployment through red team exercises, prompt injection testing, jailbreak simulations, and robustness validation using golden datasets.

---

## Why This Matters for FSI

- **OCC 2011-12:** Model validation includes stress testing and robustness assessment
- **Fed SR 11-7:** Independent model review should include adversarial scenarios
- **FINRA 25-07:** Testing evidence demonstrates due diligence for AI systems
- **NIST AI RMF:** Structured adversarial testing supports AI risk management

---

## Control Description

This control establishes adversarial testing through:

1. **Red Team Program** - Structured approach to adversarial testing with defined scope and rules
2. **Attack Library** - Curated test cases for prompt injection, jailbreaking, and manipulation
3. **Golden Dataset Validation** - Reference datasets for measuring agent accuracy and robustness
4. **CI/CD Integration** - Automated adversarial testing in deployment pipeline
5. **Evidence Documentation** - Maintain records for regulatory examination
6. **Remediation Tracking** - SLA-based remediation of identified vulnerabilities

---

## Key Configuration Points

- Establish red team program with designated team members and rules of engagement
- Create attack library covering OWASP LLM Top 10 and MITRE ATLAS techniques
- Build golden dataset with 100+ domain-specific Q&A pairs
- Integrate automated adversarial testing in pre-deployment pipeline
- Define remediation SLAs: Critical (24h), High (7d), Medium (30d)
- Schedule quarterly red team exercises for Zone 3 agents
- Preserve test evidence per retention requirements (7+ years)

---

## Zone-Specific Requirements

| Zone | Requirement | Rationale |
|------|-------------|-----------|
| **Zone 1** (Personal) | Basic prompt injection tests; pre-deployment only | Low risk, minimal adversarial exposure |
| **Zone 2** (Team) | Comprehensive test suite; quarterly testing; golden dataset validation | Shared agents warrant structured testing |
| **Zone 3** (Enterprise) | Full red team program; continuous testing; third-party assessment annually; immediate remediation | Customer-facing requires maximum adversarial validation |

---

## Roles & Responsibilities

| Role | Responsibility |
|------|----------------|
| AI Governance Lead | Red team program governance, remediation oversight |
| Security Team | Execute adversarial testing, maintain attack library |
| QA Lead | Golden dataset management, CI/CD integration |
| Agent Owner | Remediate vulnerabilities, implement fixes |

---

## Related Controls

| Control | Relationship |
|---------|--------------|
| [1.21 - Adversarial Input Logging](../pillar-1-security/1.21-adversarial-input-logging.md) | Detection complements proactive testing |
| [2.5 - Testing & Validation](2.5-testing-validation-and-quality-assurance.md) | Adversarial testing is part of validation |
| [2.6 - Model Risk Management](2.6-model-risk-management-alignment-with-occ-2011-12-sr-11-7.md) | Red team supports model validation |
| [2.11 - Bias Testing](2.11-bias-testing-and-fairness-assessment-finra-notice-25-07-sr-11-7-alignment.md) | Complementary testing approach |

---

## Implementation Guides

> For step-by-step implementation, see the playbooks:

- [Portal Walkthrough](../../playbooks/control-implementations/2.20/portal-walkthrough.md) - Step-by-step portal configuration
- [PowerShell Setup](../../playbooks/control-implementations/2.20/powershell-setup.md) - Scripts for automation
- [Verification & Testing](../../playbooks/control-implementations/2.20/verification-testing.md) - Test cases and evidence collection
- [Troubleshooting](../../playbooks/control-implementations/2.20/troubleshooting.md) - Common issues and resolutions

---

## Verification Criteria

Confirm control effectiveness by verifying:

1. Red team program documented with rules of engagement
2. Attack library covers OWASP LLM Top 10 vulnerability categories
3. Golden dataset contains representative domain-specific Q&A pairs
4. Automated adversarial testing executes in deployment pipeline
5. Remediation tracking shows SLA compliance for identified vulnerabilities

---

## Additional Resources

- [MITRE ATLAS: AI Adversarial Threat Landscape](https://atlas.mitre.org/)
- [OWASP Top 10 for LLM Applications](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
- [Microsoft AI Red Team](https://learn.microsoft.com/en-us/security/ai-red-team/)
- [NIST AI Risk Management Framework](https://www.nist.gov/itl/ai-risk-management-framework)

---

*Updated: January 2026 | Version: v1.1 | UI Verification Status: Current*
