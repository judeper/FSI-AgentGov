# Control 2.6: Model Risk Management (Alignment with OCC 2011-12/SR 11-7)

## Overview

**Control ID:** 2.6  
**Control Name:** Model Risk Management (Alignment with OCC 2011-12/SR 11-7)  
**Pillar:** Management  
**Regulatory Reference:** OCC 2011-12, Federal Reserve SR 11-7, FINRA Notice 25-07, SOX 302/404  
**Setup Time:** 4-6 hours  

### Purpose

Model Risk Management (MRM) ensures that AI agents used in financial services are subject to the same rigorous governance as traditional quantitative models. OCC Bulletin 2011-12 and Federal Reserve SR Letter 11-7 establish requirements for model development, validation, and ongoing monitoring. While Copilot Studio agents may not be "models" in the traditional sense, their use in customer-facing or decision-support roles requires similar governance to manage the risk of incorrect outputs, bias, or regulatory non-compliance.

This control addresses key FSI requirements:

- **Model Inventory**: Catalog agents that function as models
- **Independent Validation**: Third-party review of agent behavior
- **Performance Monitoring**: Track output quality over time
- **Bias Detection**: Identify and mitigate unfair outcomes
- **Documentation**: Complete model development lifecycle records
- **Change Control**: Governance for model modifications

---

## Prerequisites

**Primary Owner Admin Role:** AI Governance Lead
**Supporting Roles:** Compliance Officer, Power Platform Admin

### Required Licenses

| License | Purpose |
|---------|---------|
| Power Platform per-user | Agent development and monitoring |
| Microsoft Purview (any tier) | Audit logging for model governance |
| Azure Monitor (optional) | Advanced performance monitoring |

### Required Permissions

| Permission | Scope | Purpose |
|------------|-------|---------|
| Power Platform Admin | Tenant | Agent inventory and oversight |
| Compliance Administrator | Microsoft Purview | Audit log access |
| Model Risk Manager | Business role | Model governance |

### Dependencies
- [Control 2.5: Testing and Validation](2.5-testing-validation-and-quality-assurance.md) - Pre-deployment validation
- [Control 2.11: Bias Testing](2.11-bias-testing-and-fairness-assessment-finra-notice-25-07-sr-11-7-alignment.md) - Fairness assessment
- [Control 3.1: Agent Inventory](../pillar-3-reporting/3.1-agent-inventory-and-metadata-management.md) - Model inventory

### Pre-Setup Checklist
- [ ] Review OCC 2011-12 and SR 11-7 requirements
- [ ] Identify agents that qualify as "models"
- [ ] Establish Model Risk Management committee
- [ ] Define model tiering criteria
- [ ] Identify independent validation resources

---

## Governance Levels

### Baseline (Level 1)

Document model risk assessment for AI agents; establish validation procedures.

### Recommended (Level 2-3)

Quarterly model monitoring; bias testing; performance vs. baseline tracking.

### Regulated/High-Risk (Level 4)

Comprehensive model risk framework per SR 11-7; annual third-party validation; real-time performance monitoring.

---

## Setup & Configuration

### GPT-5 Model Availability and Governance

!!! info "GPT-5 Now Default Model"
    As of November 2025, **GPT-5** is the default model for new Copilot Studio agents. Organizations should update their Model Risk Management practices to address this new model capability.

#### Model Selection Governance

Copilot Studio now offers multiple model options with different capabilities and risk profiles:

| Model | Availability | Capability Level | FSI Recommendation |
|-------|-------------|------------------|-------------------|
| **GPT-5** | GA (Nov 2025) | Production-ready, default | Approved for all zones |
| **GPT-5.2** | Preview | Experimental capabilities | Zone 1 only; Zone 2/3 disabled |
| **GPT-4o** | GA | Previous generation | Approved for all zones |
| **Custom models** | Varies | Organization-specific | Requires MRM validation |

#### OCC 2011-12 Implications for Model Changes

When an agent's underlying model changes (including automatic updates from Microsoft), this may constitute a material model change under OCC 2011-12:

**Change Classification:**

| Scenario | Classification | Governance Requirement |
|----------|---------------|----------------------|
| New agent with GPT-5 default | Initial deployment | Standard model validation |
| Existing agent upgraded to GPT-5 | Material change | Revalidation may be required |
| Switch to experimental model | Material change | Full MRM review |
| Microsoft model updates | Monitor impact | Performance baseline comparison |

#### Model Selection Configuration

**Portal Path:** Copilot Studio → [Agent] → Settings → AI capabilities → Model

1. Navigate to **Copilot Studio** → Select agent
2. Click **Settings** → **AI capabilities**
3. Review **Model** selection
4. Document model choice in agent inventory

#### Zone-Specific Model Governance

| Zone | Allowed Models | Experimental Models | Documentation |
|------|---------------|---------------------|---------------|
| **Zone 1** | GPT-5, GPT-4o | Allowed for evaluation | Minimal |
| **Zone 2** | GPT-5, GPT-4o | Disabled | Standard MRM |
| **Zone 3** | GPT-5, GPT-4o (approved only) | **Disabled** | Full MRM with validation |

#### Model Performance Monitoring

When upgrading agents to new models, establish performance baselines:

```yaml
# Model Change Monitoring Checklist
model_change:
  agent_id: "[Agent ID]"
  previous_model: "GPT-4o"
  new_model: "GPT-5"
  change_date: "[Date]"

  baseline_metrics:
    accuracy_before: "[%]"
    accuracy_after: "[%]"
    response_time_before: "[ms]"
    response_time_after: "[ms]"
    user_satisfaction_before: "[score]"
    user_satisfaction_after: "[score]"

  validation:
    - [ ] Performance meets or exceeds baseline
    - [ ] No regression in accuracy
    - [ ] Bias testing completed (if Tier 1/2 model)
    - [ ] User acceptance testing passed
    - [ ] MRM notification submitted (if material)
```

---

### Step 1: Define Agent-as-Model Classification

**Determine Which Agents Require MRM Governance:**

1. **Model Definition per OCC 2011-12:**
   > A model is a quantitative method, system, or approach that applies statistical, economic, financial, or mathematical theories, techniques, and assumptions to process input data into quantitative estimates.

2. **Agent Classification Criteria:**

   | Criteria | Model Treatment | Example |
   |----------|-----------------|---------|
   | Makes decisions affecting customers | Yes - Tier 1 | Credit recommendation agent |
   | Provides financial calculations | Yes - Tier 1/2 | Investment calculator agent |
   | Influences risk assessments | Yes - Tier 1 | Risk scoring agent |
   | Customer-facing recommendations | Yes - Tier 2 | Product recommendation agent |
   | Information retrieval only | No | FAQ/knowledge base agent |
   | Internal productivity | No | IT help desk agent |

3. **Document Classification:**
   ```
   Agent Model Classification Form

   Agent Name: [Name]
   Agent ID: [ID]
   Business Owner: [Owner]

   Classification Decision:
   [ ] Model (requires MRM governance)
   [ ] Non-Model (standard agent governance)

   Justification:
   [Explain why agent does/doesn't qualify as model]

   Model Tier (if applicable):
   [ ] Tier 1 - High Risk (material business impact)
   [ ] Tier 2 - Medium Risk (significant but limited impact)
   [ ] Tier 3 - Low Risk (minimal business impact)

   Approved by: _________________ Date: _________
   Model Risk Manager
   ```

### Step 2: Establish Model Inventory

**Create Model Inventory Registry:**

1. **Required Inventory Fields:**

   | Field | Description |
   |-------|-------------|
   | Model ID | Unique identifier |
   | Model Name | Agent/model name |
   | Model Tier | 1, 2, or 3 |
   | Business Purpose | What the model does |
   | Model Owner | Business owner |
   | Model Developer | Technical owner |
   | Primary Users | Who uses the model |
   | Data Inputs | Data sources used |
   | Model Outputs | Decisions/recommendations |
   | Implementation Date | Go-live date |
   | Last Validation | Date of last review |
   | Next Validation Due | Scheduled review date |
   | Performance Status | Green/Yellow/Red |

2. **Create SharePoint List or Dataverse Table:**
   - Power Platform Admin Center → Create new Dataverse table
   - Or SharePoint → Create list with above columns
   - Enable version history for audit trail

### Step 3: Document Model Development

**Model Development Documentation Template:**

```markdown
# Model Development Documentation
## [Agent/Model Name]

### 1. Executive Summary
- Model purpose and business objectives
- Key stakeholders and users
- Expected benefits and risks

### 2. Model Scope and Limitations
- Intended use cases
- Populations/scenarios covered
- Known limitations
- Conditions where model should not be used

### 3. Model Design
- Conceptual design approach
- Data sources and inputs
- Processing methodology
- Output specifications
- Error handling approach

### 4. Development Process
- Development timeline
- Team members and roles
- Development environment
- Testing methodology
- Peer review documentation

### 5. Data Description
- Input data sources
- Data quality requirements
- Data preprocessing steps
- Training data (if applicable)
- Ongoing data requirements

### 6. Model Performance
- Performance metrics defined
- Baseline performance levels
- Acceptable performance thresholds
- Monitoring approach

### 7. Implementation Details
- Production environment
- Integration points
- Security controls
- Operational procedures

### 8. Validation Summary
- Initial validation results
- Ongoing validation schedule
- Validation methodology

### 9. Appendices
- Technical specifications
- Test results
- Approval documentation
```

### Step 4: Establish Validation Framework

**Independent Validation Requirements:**

1. **Validation Tiers:**

   | Model Tier | Validation Requirements | Frequency |
   |------------|------------------------|-----------|
   | Tier 1 | Independent third-party | Annual |
   | Tier 2 | Independent internal team | Annual |
   | Tier 3 | Self-assessment + review | Biennial |

2. **Validation Scope:**

   **Conceptual Soundness:**
   - [ ] Model design is appropriate for intended use
   - [ ] Methodology is theoretically sound
   - [ ] Assumptions are reasonable and documented
   - [ ] Limitations are clearly stated

   **Data Quality:**
   - [ ] Data sources are appropriate
   - [ ] Data quality is acceptable
   - [ ] Data preprocessing is appropriate
   - [ ] Data is representative of use cases

   **Output Analysis:**
   - [ ] Outputs are accurate and reliable
   - [ ] Performance meets expectations
   - [ ] Outputs are consistent over time
   - [ ] Edge cases handled appropriately

   **Implementation Verification:**
   - [ ] Model implemented as designed
   - [ ] Controls are effective
   - [ ] Documentation is complete
   - [ ] Users are properly trained

3. **Validation Report Template:**
   ```markdown
   # Model Validation Report
   ## [Model Name] - [Validation Date]

   ### Validation Scope
   - Validation type: [Initial / Annual / Ad-hoc]
   - Validator: [Name/Firm]
   - Independence statement: [Confirm no development involvement]

   ### Summary of Findings
   | Area | Finding | Severity | Recommendation |
   |------|---------|----------|----------------|
   | [Data Quality] | [Example: 2% of test cases failed validation] | [Medium] | [Implement additional input validation] |

   ### Detailed Assessment
   [Section for each validation area]

   ### Conclusion
   - Overall validation status: [Approved / Conditional / Not Approved]
   - Conditions (if any): [List conditions]
   - Next validation date: [Date]

   ### Sign-off
   Validator: _________________ Date: _________
   Model Risk Manager: _________________ Date: _________
   ```

### Step 5: Configure Performance Monitoring

**Portal Path:** Power Platform Admin Center → Analytics → Copilot Studio

1. **Define Performance Metrics:**

   | Metric | Description | Threshold |
   |--------|-------------|-----------|
   | Response Accuracy | Correct responses / Total | >95% |
   | User Satisfaction | CSAT score | >4.0/5.0 |
   | Fallback Rate | Escalations to human | <10% |
   | Response Time | Average response latency | <2 seconds |
   | Error Rate | Failed conversations | <2% |
   | Bias Indicators | Demographic disparity | <5% variance |

2. **Create Monitoring Dashboard:**
   - Use Power BI connected to Dataverse
   - Include trend charts for each metric
   - Configure alerts for threshold breaches

3. **Automated Monitoring Script:**

   ```powershell
   # Model Performance Monitoring Script
   param(
       [string]$AgentId,
       [int]$DaysToAnalyze = 30
   )

   # Connect to Dataverse
   # (Assumes connection established)

   # Query conversation logs
   $startDate = (Get-Date).AddDays(-$DaysToAnalyze)

   # Calculate metrics
   $metrics = @{
       TotalConversations = 0
       SuccessfulResolutions = 0
       Escalations = 0
       AverageResponseTime = 0
       UserSatisfactionAvg = 0
   }

   # (Query and calculate actual values)

   # Calculate performance scores
   $resolutionRate = $metrics.SuccessfulResolutions / $metrics.TotalConversations * 100
   $escalationRate = $metrics.Escalations / $metrics.TotalConversations * 100

   # Determine status
   $status = "Green"
   if ($resolutionRate -lt 90 -or $escalationRate -gt 15) {
       $status = "Yellow"
   }
   if ($resolutionRate -lt 80 -or $escalationRate -gt 25) {
       $status = "Red"
   }

   # Output report
   Write-Host "=== Model Performance Report ===" -ForegroundColor Cyan
   Write-Host "Agent: $AgentId"
   Write-Host "Period: Last $DaysToAnalyze days"
   Write-Host "Status: $status"
   Write-Host ""
   Write-Host "Metrics:"
   Write-Host "  Resolution Rate: $([math]::Round($resolutionRate, 2))%"
   Write-Host "  Escalation Rate: $([math]::Round($escalationRate, 2))%"
   Write-Host "  Avg Response Time: $($metrics.AverageResponseTime)ms"
   Write-Host "  User Satisfaction: $($metrics.UserSatisfactionAvg)/5.0"

   # Alert if threshold breached
   if ($status -ne "Green") {
       Write-Host "`nALERT: Performance threshold breached!" -ForegroundColor $status
       # Send notification (Teams, email, etc.)
   }
   ```

### Step 6: Establish Change Control for Models

**Model Change Governance:**

1. **Change Classification:**

   | Change Type | Description | Governance |
   |-------------|-------------|------------|
   | Material Change | Affects model outputs significantly | Full revalidation |
   | Non-Material Change | Minor updates, bug fixes | Abbreviated review |
   | Emergency Change | Critical fix | Expedited process |
   | **Prompt Change** | System prompt, topic prompt, or grounding instructions | See Step 6a |

2. **Material Change Examples:**
   - Changes to prompts affecting recommendations
   - New data source integration
   - Modified business logic
   - Significant performance tuning

3. **Change Request Form:**
   ```
   Model Change Request

   Model: [Model Name/ID]
   Requestor: [Name]
   Date: [Date]

   Change Description:
   [Detailed description of change]

   Change Classification:
   [ ] Material Change
   [ ] Non-Material Change
   [ ] Emergency Change

   Justification:
   [Business/technical rationale]

   Impact Assessment:
   - Users affected: [Number/Groups]
   - Output changes expected: [Description]
   - Risk level: [High/Medium/Low]

   Testing Plan:
   [How change will be tested]

   Rollback Plan:
   [How to revert if issues]

   Approvals Required:
   [ ] Model Owner
   [ ] Model Risk Manager (material changes)
   [ ] Business Owner
   ```

### Step 6a: Prompt Engineering Change Review

**Purpose:** Prompt changes can significantly alter agent behavior and outputs. This step establishes governance specifically for prompt modifications, which are increasingly common in AI agent maintenance.

**Prompt Change Categories:**

| Change Type | Description | Review Level | Examples |
|-------------|-------------|--------------|----------|
| **System Prompt** | Core agent instructions and persona | Full review + testing | Role definition, safety guardrails, output format |
| **Topic Prompt** | Topic-specific conversation logic | Standard review | FAQ responses, conversation flows |
| **Fallback Prompt** | Handling of unknown inputs | Standard review | "I don't know" responses, escalation triggers |
| **Grounding Instructions** | How agent uses knowledge sources | Full review + testing | Citation requirements, source priority |

**Prompt Change Review Checklist:**

```markdown
# Prompt Engineering Change Review

## Change Information
- **Agent Name/ID:** [Name]
- **Change Requestor:** [Name]
- **Change Date:** [Date]
- **Prompt Type:** [ ] System [ ] Topic [ ] Fallback [ ] Grounding

## Before/After Comparison
- **Current Prompt:** [Attach or reference]
- **Proposed Prompt:** [Attach or reference]
- **Summary of Changes:** [Description]

## Impact Assessment
- [ ] Does the change affect agent persona or role?
- [ ] Does the change affect safety guardrails?
- [ ] Does the change affect how agent handles sensitive topics?
- [ ] Does the change affect regulatory disclosures?
- [ ] Does the change affect how agent cites sources?
- [ ] Could the change introduce bias (proprietary, demographic, etc.)?

## Compliance Review
- [ ] No prohibited instructions (per Control 2.18)
- [ ] Required disclosures maintained
- [ ] Regulatory language preserved
- [ ] Safety guardrails intact

## Testing Requirements
- [ ] Baseline test suite executed
- [ ] Bias testing completed (if applicable)
- [ ] Edge case testing completed
- [ ] User acceptance testing completed

## Approvals
- [ ] Prompt Engineer: _______ Date: _____
- [ ] AI Governance Lead: _______ Date: _____
- [ ] Compliance (if regulatory): _______ Date: _____

## Documentation
- [ ] Change logged in version control
- [ ] Agent inventory updated
- [ ] Test results archived
```

**Prompt Version Control:**

Maintain version history of all prompts:

| Version | Date | Author | Change Summary | Approval |
|---------|------|--------|----------------|----------|
| 1.0 | 2026-01-01 | Initial | Initial deployment | Approved |
| 1.1 | 2026-01-15 | J. Smith | Added safety guardrail | Approved |
| 1.2 | 2026-02-01 | A. Jones | Updated disclosure language | Pending |

**Integration with Model Risk Management:**

- Prompt changes affecting Tier 1/2 agents require MRM notification
- Material prompt changes may trigger revalidation requirement
- Prompt testing results should be included in model documentation
- Prompt version history is part of model development documentation

### Step 7: Configure Regulatory Reporting

**Regulatory Examination Readiness:**

1. **Documentation Package:**
   - Model inventory (complete list)
   - Model development documentation
   - Validation reports
   - Performance monitoring reports
   - Change control documentation
   - Issue/finding tracking

2. **Examination Response Templates:**
   ```
   Model Risk Management Summary for Examination

   Total Models in Inventory: [Number]
   - Tier 1 (High Risk): [Number]
   - Tier 2 (Medium Risk): [Number]
   - Tier 3 (Low Risk): [Number]

   AI/Agent Models: [Number]

   Validation Status:
   - Current (validated within required period): [Number]
   - Overdue: [Number]

   Performance Status:
   - Green (meeting thresholds): [Number]
   - Yellow (watch list): [Number]
   - Red (remediation required): [Number]

   Open Findings: [Number]
   - High severity: [Number]
   - Medium severity: [Number]
   - Low severity: [Number]

   Key Issues/Remediation:
   [Summary of significant issues and status]
   ```

### Step 8: Integrate with Enterprise MRM Program

**If Organization Has Existing MRM Program:**

1. **Align with Existing Framework:**
   - Map agent governance to existing model tiers
   - Use existing validation resources
   - Integrate with model inventory system
   - Align reporting with MRM committee

2. **AI-Specific Considerations:**
   - Add AI/agent-specific validation criteria
   - Include bias testing in validation scope
   - Address generative AI output risks
   - Document AI-specific limitations

### Step 9: Multi-Agent Scenario Governance

**Purpose:** When multiple agents interact (agent-to-agent calls, orchestration, or delegation), the combined system introduces additional model risk that requires governance beyond individual agent validation.

**Multi-Agent Risk Considerations:**

| Scenario | Risk Description | Governance Requirement |
|----------|------------------|----------------------|
| **Agent Orchestration** | Primary agent delegates tasks to secondary agents | Treat orchestrated system as single model for validation |
| **Agent-to-Agent Calls** | Agent A invokes Agent B's capabilities | Document delegation chain; validate combined outputs |
| **Shared Knowledge Sources** | Multiple agents access same data | Ensure consistent data governance across agents |
| **Cascading Failures** | Error in one agent propagates to others | Design circuit breakers; document failure modes |

**Multi-Agent Model Risk Assessment:**

When evaluating multi-agent systems, assess:

1. **Delegation Chain Depth:**
   - Maximum recommended depth: 3 levels (Agent A → B → C)
   - Each additional level increases complexity and audit difficulty
   - Zone 3 agents should limit delegation to 2 levels without explicit approval

2. **Combined Risk Classification:**
   - If any agent in chain is Tier 1, treat entire chain as Tier 1
   - Document risk inheritance in model inventory
   - Validate end-to-end behavior, not just individual agents

3. **Escalation Boundaries:**
   - Define when multi-agent delegation requires human review
   - High-risk decisions (credit, suitability) should not be fully delegated
   - Document escalation triggers in agent configuration

**Multi-Agent Audit Trail Requirements:**

| Requirement | Description | Implementation |
|-------------|-------------|----------------|
| **Chain Logging** | Log complete delegation path | Include parent agent ID in each log entry |
| **Response Attribution** | Identify which agent produced each output | Tag responses with originating agent |
| **Timing Correlation** | Track timing across agent calls | Use correlation IDs across all interactions |
| **Decision Reconstruction** | Enable replay of decision logic | Store inputs/outputs at each delegation point |

**Multi-Agent Validation Approach:**

```
Multi-Agent Validation Checklist

System Name: [Orchestration Name]
Agents Involved: [List all agents in chain]
Total Delegation Depth: [Number]

1. Individual Agent Validation
   [ ] Each agent validated independently per tier
   [ ] Individual performance baselines established
   [ ] Individual bias testing completed

2. Combined System Validation
   [ ] End-to-end test scenarios executed
   [ ] Delegation paths tested under normal conditions
   [ ] Failure mode testing completed
   [ ] Combined output accuracy validated
   [ ] Combined bias testing (if customer-impacting)

3. Operational Readiness
   [ ] Circuit breakers configured
   [ ] Fallback procedures documented
   [ ] Monitoring covers full chain
   [ ] Alerting configured for cascade failures

4. Documentation
   [ ] System architecture documented
   [ ] Delegation rules documented
   [ ] Risk inheritance documented
   [ ] Combined limitations stated

Validated By: _________________ Date: _________
Model Risk Manager
```

**Zone Restrictions for Multi-Agent Scenarios:**

| Zone | Multi-Agent Allowance |
|------|----------------------|
| **Zone 1** | No agent-to-agent delegation allowed |
| **Zone 2** | Delegation within same zone only; max depth 2 |
| **Zone 3** | Cross-zone delegation with explicit approval; max depth 3 |

For Zone 3 multi-agent systems, include the complete agent chain in the model inventory as a single entry with references to component agents.

### Step 10: Explainability Testing Framework

**Purpose:** For agents that influence customer-facing decisions or financial recommendations, regulators increasingly expect organizations to explain how AI outputs are generated. This step establishes an explainability testing framework aligned with regulatory expectations (SEC robo-advisor guidance, FINRA Notice 25-07).

#### Explainability Requirements by Zone

| Zone | Explainability Requirement | Testing Level |
|------|---------------------------|---------------|
| **Zone 1** | Citation logging only | None |
| **Zone 2** | Decision reasoning documentation | Basic |
| **Zone 3** | Formal XAI testing for customer-impacting agents | Comprehensive |

#### Zone 3 Explainability Testing Scope

For agents classified as Tier 1 or Tier 2 models (per Step 1), implement the following explainability testing:

**1. Citation and Source Attribution:**

| Requirement | Description | Verification |
|-------------|-------------|--------------|
| Source Identification | Agent cites specific documents/sources used | Review 50+ sample responses |
| Citation Accuracy | Cited sources contain the referenced information | Spot-check 10% of citations |
| Confidence Indication | Agent indicates certainty level when appropriate | Pattern analysis of responses |

**2. Decision Path Documentation:**

For agents providing recommendations or assessments, document how inputs influence outputs:

```markdown
# Decision Path Documentation Template

## Agent: [Agent Name]
## Decision Type: [e.g., Product Recommendation, Risk Assessment]

### Input Factors
| Factor | Weight/Influence | Source |
|--------|-----------------|--------|
| Customer Age | Medium | User profile |
| Risk Tolerance | High | Questionnaire |
| Investment Horizon | High | User input |
| Account Balance | Low | Account data |

### Decision Logic
1. Agent evaluates [input factors]
2. Agent applies [business rules]
3. Agent generates [recommendation type]
4. Agent provides [disclaimers/limitations]

### Limitations
- Agent cannot assess [specific factors]
- Recommendations do not consider [excluded factors]
- Human review required for [specific scenarios]
```

**3. Formal XAI Testing Methods (Tier 1 Agents):**

For high-risk agents (customer-facing, decision-support), consider formal explainability testing:

| Method | Description | Applicability |
|--------|-------------|---------------|
| **Feature Attribution** | Identify which inputs most influenced the output | Agents using structured data inputs |
| **Counterfactual Analysis** | "What would change the recommendation?" | Product recommendation agents |
| **Attention Visualization** | Which parts of input the model focused on | Complex query handling |
| **Response Decomposition** | Break down how response was assembled | RAG-based agents |

**Feature Attribution Testing:**

```markdown
# Feature Attribution Test Results

## Agent: Investment Product Recommender
## Test Date: [Date]
## Tester: [Name]

### Test Scenario
Customer Profile:
- Age: 45
- Risk Tolerance: Moderate
- Investment Horizon: 15 years
- Current Assets: $500,000

Agent Recommendation: Balanced Growth Portfolio

### Attribution Analysis
Input Factor         | Contribution to Output
---------------------|----------------------
Risk Tolerance       | 35%
Investment Horizon   | 30%
Age                  | 20%
Current Assets       | 15%

### Key Finding
Primary driver of recommendation: Risk tolerance + Investment horizon (65% combined)

### Validation
- [ ] Attribution aligns with expected business logic
- [ ] No unexpected factors influencing output
- [ ] Sensitive attributes (race, gender, etc.) have zero contribution
```

**4. Counterfactual Testing:**

Test how changes to inputs affect outputs:

```markdown
# Counterfactual Test Results

## Agent: Credit Decision Support Agent
## Test Date: [Date]

### Baseline Scenario
Input: Customer with 720 credit score, $75k income, $20k existing debt
Output: Pre-approval recommended

### Counterfactual Tests
| Change | New Output | Expected? | Notes |
|--------|-----------|-----------|-------|
| Credit score → 650 | Review required | Yes | Expected threshold behavior |
| Income → $50k | Pre-approval recommended | Yes | Debt-to-income still acceptable |
| Debt → $40k | Review required | Yes | Debt-to-income exceeded threshold |
| Age 25 → 65 | No change | Yes | Age not a factor (as expected) |

### Validation
- [ ] Output changes align with documented business rules
- [ ] Protected characteristics do not influence decisions
- [ ] Threshold behaviors are consistent
```

#### Regulatory Context for Explainability

| Regulation/Guidance | Explainability Expectation |
|--------------------|---------------------------|
| **SEC Robo-Advisor Guidance** | Disclose methodology; explain how recommendations generated |
| **FINRA Notice 25-07** | Transparency in AI decision-making for customer interactions |
| **OCC 2021-18** | Explainability as component of AI risk management |
| **Fed SR 11-7** | Model documentation must include methodology explanation |
| **ECOA/Reg B** | Adverse action notices require reason disclosure |

#### Explainability Documentation Requirements

For each Tier 1/2 agent, maintain:

1. **Methodology Documentation:**
   - How the agent generates outputs
   - Data sources and their influence
   - Business rules applied
   - Known limitations

2. **Testing Evidence:**
   - Citation accuracy test results
   - Feature attribution analysis (if applicable)
   - Counterfactual test results (if applicable)
   - Bias testing results (per Control 2.11)

3. **User-Facing Disclosures:**
   - How agent reaches recommendations
   - Limitations of agent advice
   - When to consult human advisor

#### Explainability Compliance Checklist

```markdown
# Explainability Compliance Review

## Agent: [Name]
## Model Tier: [1/2/3]
## Review Date: [Date]

### Documentation
- [ ] Methodology documented
- [ ] Input factors and weights documented
- [ ] Limitations clearly stated
- [ ] User-facing disclosures reviewed by Compliance

### Testing (Tier 1/2 Only)
- [ ] Citation accuracy testing completed
- [ ] Decision path documentation current
- [ ] Counterfactual testing completed (Tier 1)
- [ ] Feature attribution analysis completed (Tier 1 if applicable)

### Regulatory Alignment
- [ ] Meets SEC robo-advisor disclosure requirements (if applicable)
- [ ] Meets FINRA 25-07 transparency expectations
- [ ] Adverse action explanations available (if applicable)

### Sign-Off
Compliance Officer: _________________ Date: _________
Model Risk Manager: _________________ Date: _________
```

---

### PowerShell Configuration


### Generate MRM Compliance Report

```powershell
# Model Risk Management Compliance Report
param(
    [string]$ModelInventoryPath,
    [string]$OutputPath = ".\MRM_Report_$(Get-Date -Format 'yyyyMMdd').html"
)

# Load model inventory
$models = Import-Csv -Path $ModelInventoryPath

# Calculate compliance metrics
$totalModels = $models.Count
$tier1 = ($models | Where-Object Tier -eq "1").Count
$tier2 = ($models | Where-Object Tier -eq "2").Count
$tier3 = ($models | Where-Object Tier -eq "3").Count

$currentDate = Get-Date
$validationCurrent = ($models | Where-Object {
    [datetime]$_.NextValidationDue -gt $currentDate
}).Count
$validationOverdue = ($models | Where-Object {
    [datetime]$_.NextValidationDue -le $currentDate
}).Count

$performanceGreen = ($models | Where-Object PerformanceStatus -eq "Green").Count
$performanceYellow = ($models | Where-Object PerformanceStatus -eq "Yellow").Count
$performanceRed = ($models | Where-Object PerformanceStatus -eq "Red").Count

# Generate HTML report
$html = @"
<!DOCTYPE html>
<html>
<head>
<title>Model Risk Management Report</title>
<style>
body { font-family: 'Segoe UI', sans-serif; margin: 20px; }
h1, h2 { color: #0078d4; }
.dashboard { display: flex; gap: 20px; flex-wrap: wrap; margin: 20px 0; }
.card { padding: 20px; background: #f3f2f1; border-radius: 8px; min-width: 150px; }
.card.green { background: #dff6dd; }
.card.yellow { background: #fff4ce; }
.card.red { background: #fed9cc; }
table { width: 100%; border-collapse: collapse; margin-top: 20px; }
th, td { padding: 10px; text-align: left; border-bottom: 1px solid #ddd; }
th { background: #0078d4; color: white; }
.overdue { color: red; font-weight: bold; }
</style>
</head>
<body>
<h1>Model Risk Management Compliance Report</h1>
<p>Report Date: $(Get-Date -Format 'MMMM dd, yyyy')</p>

<h2>Model Inventory Summary</h2>
<div class="dashboard">
<div class="card"><h3>Total Models</h3><p style="font-size:28px;">$totalModels</p></div>
<div class="card"><h3>Tier 1 (High)</h3><p style="font-size:28px;">$tier1</p></div>
<div class="card"><h3>Tier 2 (Medium)</h3><p style="font-size:28px;">$tier2</p></div>
<div class="card"><h3>Tier 3 (Low)</h3><p style="font-size:28px;">$tier3</p></div>
</div>

<h2>Validation Status</h2>
<div class="dashboard">
<div class="card green"><h3>Current</h3><p style="font-size:28px;">$validationCurrent</p></div>
<div class="card red"><h3>Overdue</h3><p style="font-size:28px;">$validationOverdue</p></div>
</div>

<h2>Performance Status</h2>
<div class="dashboard">
<div class="card green"><h3>Green</h3><p style="font-size:28px;">$performanceGreen</p></div>
<div class="card yellow"><h3>Yellow</h3><p style="font-size:28px;">$performanceYellow</p></div>
<div class="card red"><h3>Red</h3><p style="font-size:28px;">$performanceRed</p></div>
</div>

<h2>Model Details</h2>
<table>
<tr><th>Model ID</th><th>Name</th><th>Tier</th><th>Owner</th><th>Last Validation</th><th>Next Due</th><th>Status</th></tr>
$(
$models | ForEach-Object {
    $overdueClass = if ([datetime]$_.NextValidationDue -le $currentDate) { "overdue" } else { "" }
    "<tr><td>$($_.ModelID)</td><td>$($_.ModelName)</td><td>$($_.Tier)</td><td>$($_.ModelOwner)</td><td>$($_.LastValidation)</td><td class='$overdueClass'>$($_.NextValidationDue)</td><td>$($_.PerformanceStatus)</td></tr>"
}
)
</table>
</body>
</html>
"@

$html | Out-File -FilePath $OutputPath -Encoding UTF8
Write-Host "MRM Report generated: $OutputPath" -ForegroundColor Green
```

---

## Financial Sector Considerations

### Regulatory Alignment

| Regulation | Requirement | MRM Implementation |
|------------|-------------|-------------------|
| **OCC 2011-12** | Model risk management framework | Agent-as-model governance |
| **Fed SR 11-7** | Model validation and monitoring | Independent validation program |
| **FINRA 25-07** | AI fairness and transparency | Bias testing and documentation |
| **SOX 302/404** | Internal controls | Documented model controls |
| **OCC 2021-18** | AI/ML risk management | AI-specific governance |

### Zone-Specific Configuration

| Configuration | Zone 1 | Zone 2 | Zone 3 |
|---------------|--------|--------|--------|
| **MRM Applicability** | Non-model | May be model | Likely model |
| **Validation Type** | N/A | Internal | Independent third-party |
| **Validation Frequency** | N/A | Annual | Annual + ongoing |
| **Performance Monitoring** | Basic | Standard | Real-time |
| **Documentation Level** | Basic | Comprehensive | Comprehensive |
| **Change Governance** | Standard | Enhanced | Formal CAB |

### FSI Use Case Example

**Scenario:** Investment Recommendation Agent

**MRM Classification:**

- Tier 1 Model (material business impact)
- Provides investment product recommendations
- Influences customer financial decisions

**MRM Implementation:**

1. **Documentation:**
   - Complete model development documentation
   - Input data: Customer profile, risk tolerance, investment goals
   - Output: Product recommendations
   - Limitations: Not personalized financial advice

2. **Validation:**
   - Annual third-party validation
   - Quarterly performance review
   - Bias testing for demographic fairness
   - Output analysis for recommendation quality

3. **Monitoring:**
   - Real-time accuracy tracking
   - User satisfaction monitoring
   - Escalation rate tracking
   - Compliance sampling

4. **Governance:**
   - Changes reviewed by Model Risk Committee
   - Material changes require revalidation
   - Quarterly reporting to MRM committee

---

## Verification & Testing

### Verification Steps

1. **Classification Verification:**
   - [ ] All agents reviewed for model classification
   - [ ] Model/non-model decisions documented
   - [ ] Model tier assignments justified

2. **Documentation Verification:**
   - [ ] Model development docs complete
   - [ ] Validation reports current
   - [ ] Change control documentation maintained

3. **Monitoring Verification:**
   - [ ] Performance dashboards operational
   - [ ] Alerts configured
   - [ ] Regular reporting in place

4. **Governance Verification:**
   - [ ] MRM committee oversight
   - [ ] Validation schedule maintained
   - [ ] Regulatory reporting ready

### Compliance Checklist

- [ ] Agent-as-model classification completed
- [ ] Model inventory maintained
- [ ] Validation program established
- [ ] Performance monitoring active
- [ ] Change control process documented
- [ ] Regulatory examination package ready

---

## Troubleshooting & Validation

<a id="troubleshooting"></a>


### Issue 1: Unclear Model Classification

**Symptoms:** Difficulty determining if agent is a "model"

**Resolution:**

1. Review OCC 2011-12 model definition
2. Assess if agent provides quantitative estimates
3. Evaluate impact on business decisions
4. Consult Model Risk Management team
5. When in doubt, treat as model

### Issue 2: Limited Validation Resources

**Symptoms:** Cannot perform independent validation

**Resolution:**

1. Identify internal teams not involved in development
2. Consider second-line risk functions
3. Engage external validators for Tier 1
4. Use automated validation tools
5. Document resource constraints

### Issue 3: Performance Data Unavailable

**Symptoms:** Cannot measure model performance

**Resolution:**

1. Enable conversation logging
2. Configure Dataverse analytics
3. Implement user feedback collection
4. Create manual sampling process
5. Document data limitations

---

## Additional Resources

<a id="microsoft-learn-references"></a>


- [OCC Bulletin 2011-12](https://www.occ.gov/news-issuances/bulletins/2011/bulletin-2011-12.html)
- [Federal Reserve SR 11-7](https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm)
- [Copilot Studio analytics](https://learn.microsoft.com/en-us/microsoft-copilot-studio/analytics-overview)
- [Power Platform CoE analytics](https://learn.microsoft.com/en-us/power-platform/guidance/coe/power-bi-monitor)

---

## Related Controls

| Control ID | Control Name | Relationship |
|------------|--------------|--------------|
| 2.5 | Testing and Validation | Pre-deployment validation |
| 2.11 | Bias Testing | Fairness assessment |
| 3.1 | Agent Inventory | Model inventory |
| 3.3 | Compliance Reporting | MRM reporting |

---

## Support & Questions

For implementation support or questions about this control, contact:

- **AI Governance Lead** (governance direction)
- **Compliance Officer** (regulatory requirements)
- **Technical Implementation Team** (platform setup)
---

**Updated:** Jan 2026
**Version:** v1.0 (Jan 2026)
**UI Verification Status:** ❌ Needs verification
